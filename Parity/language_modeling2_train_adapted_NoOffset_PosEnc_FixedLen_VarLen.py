from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification, GPT2Config, TrainingArguments, Trainer, TrainerCallback
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, IterableDataset
import numpy as np
import random
from copy import deepcopy
import string
import argparse
import itertools
# from utils import*
import os
from collections import Counter
from typing import Optional

class NoPE(nn.Module):
    def __init__(self) -> None:
        super().__init__()
    
    def forward(self, x):
        return 0

class NoPEGPT2LMHeadModel(GPT2LMHeadModel):
    def __init__(self, config):
        super().__init__(config)
        self.transformer.wpe = NoPE()

class RegGPT2LMHeadModel(GPT2LMHeadModel):
    def __init__(self, config, coef):
        super().__init__(config)
        self.coef = coef

    def forward(self, *args, labels: Optional[torch.LongTensor] = None, **kwargs):
        outputs = super().forward(*args, labels=labels, **kwargs)

        if labels is not None:
            loss2 = self.compute_regularizer()

            if isinstance(outputs, tuple):
                outputs = (outputs[0] + loss2 * self.coef,) + outputs[1:]
            else:
                outputs.loss = outputs.loss + loss2 * self.coef
        return outputs
    
    def compute_regularizer(self):
        pe = self.transformer.wpe.weight # (num_embeddings, embedding_dim)

        square_sum = 0
        for block in self.transformer.h:
            w_matrix = block.attn.c_attn.weight # W_qkv for this layer (including all heads), 
            # it can first be split (by columns) into 3 equal part, correspond to q, k, v. Each part then be spit into many parts for each head
            k_offset = block.attn.embed_dim
            head_dim = block.attn.head_dim
            for i in range(block.attn.num_heads):
                w_query = w_matrix[:, i*head_dim : (i+1)*head_dim]  # W_q for head i
                w_key = w_matrix[:, k_offset+i*head_dim : k_offset+(i+1)*head_dim]  # W_k for head i

                product = (pe @ w_query) @ ((pe @ w_key).T)
                product = (torch.tril(product)**2).sum(dim=0).mean()
                square_sum = square_sum + product

        return square_sum

class customTokenizer():
    def __init__(self, vocab: list[str]):
        normal_tkn_num = len(vocab) # each element is a token

        self.bos_token = "<bos>"
        self.sep_token = "<sep>"
        self.eos_token = "<eos>"
        self.pad_token = "<pad>"
        self.bos_token_id = normal_tkn_num
        self.sep_token_id = normal_tkn_num + 1
        self.eos_token_id = normal_tkn_num + 2
        self.pad_token_id = normal_tkn_num + 3
        self.special_token_ids = [self.bos_token_id, self.sep_token_id, self.eos_token_id, self.pad_token_id]
        self.special_tokens = [self.bos_token, self.sep_token, self.eos_token, self.pad_token]
        assert all(t not in vocab for t in self.special_tokens)
        
        # self.vocab = {"0": 0, "1": 1}
        self.vocab = {t: i for i, t in enumerate(vocab)}
        self.vocab[self.bos_token] = self.bos_token_id
        self.vocab[self.sep_token] = self.sep_token_id
        self.vocab[self.eos_token] = self.eos_token_id
        self.vocab[self.pad_token] = self.pad_token_id

        self.vocab_inv = {v: k for k, v in self.vocab.items()}
        self.padding_side = "right"

    def __call__(self, strings: list[str] | str, **kwargs):
        # this func is not used, since the data generator does not generate str
        # string is tokenized by white space
        if type(strings) == str:
            strings = [strings]
        ids = []
        strings = [s.split(" ") for s in strings]
        max_len = max(map(lambda x: len(x), strings))
        for s in strings:
            ids.append( list(map(lambda x: self.vocab[x], s)) + [self.pad_token_id] * (max_len-len(s)) )

        return {"input_ids": torch.LongTensor(ids)}

    def convert_ids_to_tokens(self, ids: list[int], rm_special=False):
        if rm_special:
            return [self.vocab_inv[i] for i in ids if i not in self.special_token_ids]
        else:
            return list(map(lambda x: self.vocab_inv[x], ids))

    def __len__(self):
        return len(self.vocab)
    
class BinaryMajorityDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        assert len(tokenizer) == 6
        self.range_min, self.range_max = length_range
        self.range_min = max(1, self.range_min)
        self.max_test_length = max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)   # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = random.randint(self.range_min, self.range_max)
            while True:
                num_zero = random.randint(0, length)
                if num_zero != length-num_zero:
                    break
            instance = [0, ] * num_zero + [1, ] * (length - num_zero)
            random.shuffle(instance)
            ans = 0 if num_zero > length-num_zero else 1

            instance.insert(0, self.tokenizer.bos_token_id)
            instance.append(self.tokenizer.sep_token_id)
            instance.append(ans)
            instance.append(self.tokenizer.eos_token_id)

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 
            
            if self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label


class MajorityDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer
        self.range_min, self.range_max = length_range
        self.range_min = max(1, self.range_min)
        self.max_test_length = max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)   # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = random.randint(self.range_min, self.range_max)
            while True:
                instance = random.choices(range(len(self.tokenizer)-4), k=length)
                most_common = Counter(instance).most_common(2)
                if len(most_common) < 2 or most_common[0][1] > most_common[1][1]:
                    break
            ans = most_common[0][0]

            instance.insert(0, self.tokenizer.bos_token_id)
            instance.append(self.tokenizer.sep_token_id)
            instance.append(ans)
            instance.append(self.tokenizer.eos_token_id)

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 
            
            if self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label

class BinaryMajorityInterleaveSepDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int, period: int):
        super().__init__()
        self.tokenizer = tokenizer 
        assert len(tokenizer) == 6
        self.range_min, self.range_max = length_range
        self.range_min = max(3, self.range_min)
        self.max_test_length = max_test_length
        self.period = period
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            total_length = random.randint(self.range_min, self.range_max)
            length = round(total_length / self.period)
            if length * self.period > self.range_max:
                length -= 1
            if length * self.period < self.range_min:
                length += 1
            
            instances = []
            answers = []
            for i in range(self.period):
                while True:
                    num_zero = random.randint(0, length)
                    if num_zero != length-num_zero:
                        break
                instance = [0, ] * num_zero + [1, ] * (length - num_zero)
                random.shuffle(instance)
                instances.append(instance)

                ans = 0 if num_zero > length-num_zero else 1
                answers.append(ans)

            whole_instance = [val for tup in zip(*instances) for val in tup]

            whole_instance.insert(0, self.tokenizer.bos_token_id)
            whole_instance.append(self.tokenizer.sep_token_id)
            whole_instance.extend(answers)
            whole_instance.append(self.tokenizer.eos_token_id)

            label = deepcopy(whole_instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length*self.period+2] = [self.tokenizer.pad_token_id,] * (length*self.period+2)   # bos + bits.. + sep 
            
            if self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length*self.period)
            else:
                offset = 0
            pos_ids = list(range(offset, len(whole_instance)+offset))

            yield whole_instance, pos_ids, label


class UniqueCopyDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.range_min = max(1, self.range_min)
        self.max_test_length = max_test_length
        assert len(tokenizer) - 4 >= max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = random.randint(self.range_min, self.range_max)     # length of string to be copied
            
            temp = random.sample(range(len(self.tokenizer)-4), length)
            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            instance.extend(temp)
            instance.append(self.tokenizer.eos_token_id)

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + ... + sep 
            
            if self.max_test_length != -1:
                offset = random.randint(0, (self.max_test_length - length) * 2)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label


class UniqueCopyRepeatDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.range_min = max(2, self.range_min)
        self.max_test_length = max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length
        
        def find_factors(n):
            lis = []
            for i in range(1, n+1):
                if n % i == 0:
                    if 2 <= (n // i) <= (len(self.tokenizer) - 3):
                        lis.append(i)
            return lis
        
        self.n_repeat = {}
        for i in range(self.range_min, self.range_max+1):
            lis = find_factors(i) 
            if lis:
                self.n_repeat[i] = lis
        print(self.n_repeat)

    def __iter__(self):
        while True:
            length = random.choice(list(self.n_repeat.keys()))
            
            possible_n_repeat = self.n_repeat[length]
            num_repeat = random.choice(possible_n_repeat)
            unit_length = length // num_repeat

            temp = random.sample(range(len(self.tokenizer)-4), unit_length-1)
            temp.append(self.tokenizer.sep_token_id)

            instance = [self.tokenizer.bos_token_id]
            for _ in range(num_repeat+1):
                instance.extend(temp)
            instance[-1] = self.tokenizer.eos_token_id

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+1] = [self.tokenizer.pad_token_id,] * (length+1)   # bos + ... + sep + ... + sep
            
            if self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length + len(self.tokenizer) - 2 - len(instance))
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label


class RepeatCopyDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.range_min = max(1, self.range_min)
        self.max_test_length = max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = random.randint(self.range_min, self.range_max)     # length of string to be copied
            
            temp = random.choices(range(len(self.tokenizer)-4), k=length)
            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            instance.extend(temp)
            instance.append(self.tokenizer.eos_token_id)

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + ... + sep 
            
            if self.max_test_length != -1:
                offset = random.randint(0, (self.max_test_length - length) * 2)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label




class SortDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.range_min = max(1, self.range_min)
        self.max_test_length = max_test_length
        assert len(tokenizer) - 4 >= max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = random.randint(self.range_min, self.range_max)     # length of string to be copied

            temp = random.sample(range(len(self.tokenizer)-4), length)
            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            instance.extend(sorted(temp))
            instance.append(self.tokenizer.eos_token_id)

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 
            
            if self.max_test_length != -1:
                offset = random.randint(0, (self.max_test_length - length) * 2)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label


class MedianUnique2Dataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int, COT : int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.max_test_length = max_test_length
        self.COT = COT
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = self.range_max
            numLen = math.ceil(1+math.log(10*length)/math.log(10))
            numbers = random.sample(range(10*length + 1), length) #[random.randint(0,length-1) for _ in range(length)]
#            print(numbers)
            sorted_numbers = sorted(numbers)
            median = sorted_numbers[length//2]
            if (length//2)//self.COT == 0:
                print("!!!!", length, length//2, self.COT, (length//2)//self.COT)
            cot = [sorted_numbers[i] for i in range(0, length//2, (length//2)//self.COT)]
            temp = numbers
            temp = []
            for i in numbers:
                i_ = str(i)
                i_ = ("0"*(numLen - len(i_))) + i_
                assert len(i_) == numLen, (i_, length)
                for j in i_:
                    temp.append(int(j)+1)
#            print([x-1 for x in temp])
            ans = []
            for i in cot + [median]:
                i_ = str(i)
                i_ = ("0"*(numLen - len(i_))) + i_
                assert len(i_) == numLen, i_
                for j in i_:
                    ans.append(int(j)+1)

            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            #instance.append(ans)
            #instance.append(self.tokenizer.eos_token_id)
            label = ans

            #label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            #label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 

            
            if False: #self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+len(ans)+offset))
            #print(instance)
            yield instance, pos_ids, label


class MedianUniqueDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int, COT : int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.max_test_length = max_test_length
        self.COT = COT
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = self.range_max
            numLen = math.ceil(2*math.log(length)/math.log(10))
            numbers = random.sample(range(2*length + 1), length) #[random.randint(0,length-1) for _ in range(length)]
            sorted_numbers = sorted(numbers)
            median = sorted_numbers[length//2]
            cot = [sorted_numbers[i] for i in range(0, length//2, (length//2)//self.COT)]
            temp = numbers
            temp = []
            for i in numbers:
                i_ = str(i)
                i_ = ("0"*(numLen - len(i_))) + i_
                assert len(i_) == numLen, (i_, length)
                for j in i_:
                    temp.append(int(j)+1)
            ans = []
            for i in cot + [median]:
                i_ = str(i)
                i_ = ("0"*(numLen - len(i_))) + i_
                assert len(i_) == numLen, i_
                for j in i_:
                    ans.append(int(j)+1)

            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            #instance.append(ans)
            #instance.append(self.tokenizer.eos_token_id)
            label = ans

            #label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            #label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 

            
            if False: #self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+len(ans)+offset))
            #print(instance)
            yield instance, pos_ids, label


class MedianDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int, COT : int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.max_test_length = max_test_length
        self.COT = COT
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = self.range_max
            numLen = math.ceil(math.log(length)/math.log(10))
            numbers = [random.randint(0,length-1) for _ in range(length)]
            sorted_numbers = sorted(numbers)
            median = sorted_numbers[length//2]
            cot = [sorted_numbers[i] for i in range(0, length//2, (length//2)//self.COT)]
            temp = numbers
            temp = []
            for i in numbers:
                i_ = str(i)
                i_ = ("0"*(numLen - len(i_))) + i_
                assert len(i_) == numLen, (i_, length)
                for j in i_:
                    temp.append(int(j)+1)
            ans = []
            for i in cot + [median]:
                i_ = str(i)
                i_ = ("0"*(numLen - len(i_))) + i_
                assert len(i_) == numLen, i_
                for j in i_:
                    ans.append(int(j)+1)

            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            #instance.append(ans)
            #instance.append(self.tokenizer.eos_token_id)
            label = ans

            #label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            #label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 

            
            if False: #self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+len(ans)+offset))
            #print(instance)
            yield instance, pos_ids, label



class ParityDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.max_test_length = max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = self.range_max
            num_ones = random.randint(0, length)
            temp = [self.tokenizer.vocab["1"]] * num_ones + [self.tokenizer.vocab["0"]] * (length - num_ones)
            random.shuffle(temp)
            ans = self.tokenizer.vocab["e"] if num_ones % 2 == 0 else self.tokenizer.vocab["o"]

            instance = [self.tokenizer.bos_token_id]
            instance.extend(temp)
            instance.append(self.tokenizer.sep_token_id)
            #instance.append(ans)
            #instance.append(self.tokenizer.eos_token_id)
            label = ans

            #label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            #label[:length+2] = [self.tokenizer.pad_token_id,] * (length+2)   # bos + bits.. + sep 

            
            if False: #self.max_test_length != -1:
                offset = random.randint(0, self.max_test_length - length)
            else:
                offset = 0
            pos_ids = list(range(offset, len(instance)+offset))
            #print(instance)
            yield instance, pos_ids, label

class AdditionDataset(IterableDataset):
    def __init__(self, tokenizer: customTokenizer, length_range: tuple[int, int], max_test_length: int):
        super().__init__()
        self.tokenizer = tokenizer 
        self.range_min, self.range_max = length_range
        self.range_min = max(4, self.range_min)
        self.max_test_length = max_test_length
        assert (max_test_length >= self.range_max) or (max_test_length == -1)    # the pos emb is initialized based on max_test_length

    def __iter__(self):
        while True:
            length = random.randint(self.range_min, self.range_max)     # length of string to be copied

            len_operand1 = random.randint(1, length-3)
            len_operand2 = length - 2 - len_operand1
            
            if len_operand1 > 1:
                operand1 = ["1"] + random.choices(["0", "1"], k=len_operand1-1)
            else:
                operand1 = random.choices(["0", "1"], k=1)
            if len_operand2 > 1:
                operand2 = ["1"] + random.choices(["0", "1"], k=len_operand2-1)
            else:
                operand2 = random.choices(["0", "1"], k=1)

            ans = int("0b" + "".join(operand1), 2) + int("0b" + "".join(operand2), 2)
            ans = list(bin(ans)[2:])

            instance = [self.tokenizer.bos_token]
            instance.extend(operand1)
            instance.append("+")
            instance.extend(operand2)
            instance.append("=")
            instance.extend(ans)
            instance.append(self.tokenizer.eos_token)

            instance = list(map(lambda x: self.tokenizer.vocab[x], instance))

            label = deepcopy(instance)
            # setting some tokens to [pad] will make the loss on these tokens (as pred targets) be ignored
            label[:length+1] = [self.tokenizer.pad_token_id,] * (length+1)   # bos + bits.. + sep 
            
            offset = 0
#            if self.max_test_length != -1 :
 #               offset = random.randint(0, self.max_test_length*2 - len(instance))
  #          else:
   #             offset = 0
            pos_ids = list(range(offset, len(instance)+offset))

            yield instance, pos_ids, label


class EvalDataset(Dataset):
    def __init__(self, d: IterableDataset, num_data: int) -> None:
        super().__init__()
        self.data = []
        for i, item in enumerate(d):
            if i >= num_data:
                break
            self.data.append(item)

    def __getitem__(self, index):
        return self.data[index]
    
    def __len__(self):
        return len(self.data)

class customCollator():
    def __init__(self, pad_id):
        self.pad_id = pad_id

    def __call__(self, examples):
        input_ids, pos_ids, labels = tuple(zip(*examples))
        max_len = max(len(item) for item in input_ids)

        input_ids = [([self.pad_id,] * (max_len - len(item))) + item for item in input_ids]
        input_ids = torch.LongTensor(input_ids)
        #[item.extend([self.pad_id,] * (max_len - len(item))) for item in labels]
        labels = torch.LongTensor(labels)
        labels[labels == self.pad_id] = -100
        pos_ids = [([item[0],] * (max_len - len(item))) + item for item in pos_ids]
        pos_ids = torch.LongTensor(pos_ids)
        
        batch = {"input_ids": input_ids, "position_ids": pos_ids, "labels": labels}
        return batch
    
def compute_metrics(eval_preds):
   # logits, labels = eval_preds
  #  print(logits.shape, labels.shape)
 #   print(logits, labels)
#    correct = ((logits == labels)).detach().numpy()
    return {"acc": 0.5}

class myCallback(TrainerCallback):
    def on_evaluate(self, state, args, control, metrics=None, logs=None, eval_dataloader=None, **kwargs):
        assert metrics["epoch"] >= getattr(self, "current_epoch", 0)
        if metrics["epoch"] > getattr(self, "current_epoch", 0):
            self.latest_acc = {}
            self.current_epoch = metrics["epoch"]
        for key in metrics.keys():
            if key.endswith("acc"):
                self.latest_acc[key] = metrics[key]
        if len(self.latest_acc) == len(test_length_ranges):
            if (self.latest_acc[f"eval_len{train_length_range[0]}-{train_length_range[1]}_acc"] == 1.0) or (self.current_epoch == 1.0):  
                if self.latest_acc[f"eval_len{train_length_range[0]}-{train_length_range[1]}_acc"] == 1.0: 
                    control.should_training_stop = True
                    global fit_train_data
                    fit_train_data = True
                    msg = f"early stop {self.current_epoch}\t\t"
                else:
                    msg = "reach max step\t\t"
                if self.latest_acc[f"eval_len{train_length_range[0]}-{train_length_range[1]}_acc"] >= 0.99:
                    msg = ">> " + msg
                print(f"{n_layer}l{n_head}h{d_model}d\t\t", msg, "\t\t".join([f"{k}: {v}" for k, v in self.latest_acc.items()]), f"\t\tlr: {lr}", file=summary_f)
                summary_f.flush()

                if (self.latest_acc[f"eval_len{train_length_range[0]}-{train_length_range[1]}_acc"] == 1.0) and (self.latest_acc[f"eval_len{test_length_ranges[1][0]}-{test_length_ranges[1][1]}_acc"] == 1.0):
                    global should_stop
                    should_stop = True
                


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--task", type=str, choices=["bin_majority", "majority", "bin_majority_interleave", "bin_majority_interleave_sep", "unique_copy", "unique_copy_repeat", "repeat_copy", "sort", "parity", "addition"])
    parser.add_argument("--nope", action="store_true")
    parser.add_argument("--regularize", type=float, default=0.0)
    parser.add_argument("--nostop", action="store_true")
    args = parser.parse_args()

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    torch.manual_seed(0)
    random.seed(0)

    train_length_range = (0, 50)
    test_length_ranges = [train_length_range] + [(51, 100), (101, 150)]
    max_test_length = test_length_ranges[-1][1]
    batch_size = 64
    per_device_bz = batch_size // torch.cuda.device_count() if torch.cuda.is_available() else batch_size 
    test_num = 2_000

    configs = [(l, h, d, lr) for l in [1, 2, 4] for h in [1, 2, 4] for d in [16, 64, 256] for lr in [1e-3, 1e-4]]
    # configs.append((12, 12, 768, 1e-4))
    # configs = [(12, 12, 768, 1e-4)]

    if args.task == "bin_majority":
        tokenizer = customTokenizer(["0", "1"])
        train_dataset = BinaryMajorityDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(BinaryMajorityDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + 4      # bos, sep, ans, eos

    elif args.task == "majority":
        tokenizer = customTokenizer(list(string.ascii_lowercase))
        train_dataset = MajorityDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(MajorityDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + 4      # bos, sep, ans, eos

    elif args.task == "bin_majority_interleave_sep":
        tokenizer = customTokenizer(["0", "1"])
        train_dataset = BinaryMajorityInterleaveSepDataset(tokenizer, train_length_range, max_test_length, period=3)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(BinaryMajorityInterleaveSepDataset(tokenizer, test_range, -1, 3), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + 6    # ans

    elif args.task == "unique_copy":
        tokenizer = customTokenizer([str(i) for i in range(max_test_length)])
        train_dataset = UniqueCopyDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(UniqueCopyDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length*2 + 3  # bos, sep, eos
    
    elif args.task == "unique_copy_repeat":
        tokenizer = customTokenizer(list(string.ascii_lowercase))
        train_dataset = UniqueCopyRepeatDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(UniqueCopyRepeatDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + len(tokenizer) - 2  # bos, eos

    elif args.task == "repeat_copy":
        tokenizer = customTokenizer(["a", "b"])
        train_dataset = RepeatCopyDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(RepeatCopyDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length*2 + 3  # bos, sep, eos

    elif args.task == "sort":
        tokenizer = customTokenizer([str(i) for i in range(max_test_length)])
        train_dataset = SortDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(SortDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length*2 + 3  # bos, sep, eos

    elif args.task == "median_unique":
        tokenizer = customTokenizer(["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"])       # even, odd
        train_dataset = MedianUniqueDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(MedianUniqueDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + 4  # bos, sep, ans, eos

    elif args.task == "median":
        tokenizer = customTokenizer(["0", "1", "2", "3", "4", "5", "6", "7", "8", "9"])       # even, odd
        train_dataset = MedianDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(MedianDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + 4  # bos, sep, ans, eos

    elif args.task == "parity":
        tokenizer = customTokenizer(["0", "1", "e", "o"])       # even, odd
        train_dataset = ParityDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(ParityDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length + 4  # bos, sep, ans, eos

    elif args.task == "addition":
        tokenizer = customTokenizer(["0", "1", "+", "="])      
        train_dataset = AdditionDataset(tokenizer, train_length_range, max_test_length)

        test_dataset = {
            f"len{test_range[0]}-{test_range[1]}": EvalDataset(AdditionDataset(tokenizer, test_range, -1), test_num)
                for test_range in test_length_ranges
        }

        n_positions = max_test_length*2  # bos, ans, eos
    else:
        assert False
    
    task_path = f"./lm-out-new-{args.task}{'-full' if args.nostop else ''}"
    if not os.path.exists(task_path):
        os.mkdir(task_path)
    if args.nope:
        suffix = "-nope"
    elif args.regularize != 0:
        suffix = f"-reg{args.regularize}"
    else:
        suffix = ""
    summary_f = open(os.path.join(task_path, f"summary{suffix}.txt"), "w")
    # summary_f = open(os.path.join(task_path, f"bigmodel{suffix}.txt"), "w")

    for i in range(3):
        print("\ninput example:")
        print(" ".join(tokenizer.convert_ids_to_tokens(test_dataset[f"len{test_length_ranges[0][0]}-{test_length_ranges[0][1]}"][i][0])))
        print("label example:")
        print(" ".join(tokenizer.convert_ids_to_tokens(test_dataset[f"len{test_length_ranges[0][0]}-{test_length_ranges[0][1]}"][i][2])))

    should_stop = False
    fit_train_data = False
    for n_layer, n_head, d_model, lr in configs: 

        if n_layer > 4:
            max_steps = 60_000
            warmup_steps = 3000
            if fit_train_data:
                break
        else:
            max_steps = 30_000
            warmup_steps = 0

        output_dir = f"{n_layer}l{n_head}h{d_model}d{'smalllr' if lr == 1e-4 else ''}{suffix}"
        output_dir = os.path.join(task_path, output_dir)

        cfg = GPT2Config(vocab_size=len(tokenizer), 
                    n_positions=n_positions,
                    n_embd=d_model,
                    n_layer=n_layer,
                    n_head=n_head,
                    bos_token_id=tokenizer.bos_token_id, 
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id,
                    attn_pdrop=0,
                    resid_pdrop=0,
                    embd_pdrop=0,
                    )

        if args.nope:
            model = NoPEGPT2LMHeadModel(cfg)
        elif args.regularize != 0:
            model = RegGPT2LMHeadModel(cfg, args.regularize)
        else:
            model = GPT2LMHeadModel(cfg)

        training_args = TrainingArguments(
            output_dir=output_dir,    
            overwrite_output_dir=True,
            per_device_train_batch_size=per_device_bz,
            per_device_eval_batch_size=per_device_bz,
            max_steps=max_steps,
            evaluation_strategy="steps",
            eval_steps=3_000,
            save_strategy="no",
            logging_strategy="steps",
            logging_steps=3_000,
            learning_rate=lr,
            weight_decay=0.01,
            optim='adamw_torch',
            lr_scheduler_type='linear',
            warmup_steps=warmup_steps,
            report_to="none",
        )

        data_collator = customCollator(tokenizer.pad_token_id)

        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
            callbacks=[myCallback],
        )

        trainer.train()
        # except:
        #     print(f"{n_layer}l{n_head}h{d_model}d\t\t", "error", f"\t\tlr: {lr}", file=summary_f)
        #     summary_f.flush()
        #     continue

        if should_stop and (not args.nostop):
            break

    

    summary_f.close()

# "bin_majority", "majority", "bin_majority_interleave", "unique_copy", "repeat_copy", "sort", "parity", "addition"
# CUDA_VISIBLE_DEVICES=3 python language_modeling2_train.py --task addition --nope
